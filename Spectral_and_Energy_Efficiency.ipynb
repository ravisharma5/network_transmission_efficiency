{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cf1b0e-86f6-4627-960e-2c058dd1bbb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/app-root/lib64/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb7df77-b571-457b-8346-20ac767b3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "import itertools\n",
    "\n",
    "# Set numpy print options\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684087f3-66ed-44f6-bc73-b8bc25dbd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Utility functions for location generation\n",
    "\n",
    "def loc_init(Size_area, Dist_TX_RX, Num_D2D, Num_Ch):\n",
    "    \"\"\"\n",
    "    Generate initial locations for D2D users and CUE.\n",
    "\n",
    "    Parameters:\n",
    "        Size_area (float): Size of the area.\n",
    "        Dist_TX_RX (float): Minimum distance between TX and RX.\n",
    "        Num_D2D (int): Number of D2D users.\n",
    "        Num_Ch (int): Number of channels.\n",
    "\n",
    "    Returns:\n",
    "        rx_loc (np.ndarray): Receiver locations.\n",
    "        tx_loc (np.ndarray): Transmitter locations.\n",
    "        tx_loc_CUE (np.ndarray): CUE transmitter locations.\n",
    "    \"\"\"\n",
    "    tx_loc = Size_area * (np.random.rand(Num_D2D, 2) - 0.5)\n",
    "    rx_loc = np.zeros((Num_D2D + 1, 2))\n",
    "    for i in range(Num_D2D):\n",
    "        temp_chan = Feasible_Loc_Init(tx_loc[i, :], Size_area, Dist_TX_RX)\n",
    "        rx_loc[i, :] = temp_chan\n",
    "    tx_loc_CUE = Size_area * (np.random.rand(Num_Ch, 2) - 0.5)\n",
    "    return rx_loc, tx_loc, tx_loc_CUE\n",
    "\n",
    "\n",
    "def Feasible_Loc_Init(Cur_loc, Size_area, Dist_TX_RX):\n",
    "    \"\"\"\n",
    "    Check the feasibility of a generated location.\n",
    "\n",
    "    Parameters:\n",
    "        Cur_loc (np.ndarray): Current location.\n",
    "        Size_area (float): Size of the area.\n",
    "        Dist_TX_RX (float): Minimum distance between TX and RX.\n",
    "\n",
    "    Returns:\n",
    "        temp_chan (np.ndarray): Feasible channel location.\n",
    "    \"\"\"\n",
    "    temp_dist = 2 * Dist_TX_RX * (np.random.rand(1, 2) - 0.5)\n",
    "    temp_chan = Cur_loc + temp_dist\n",
    "    while (np.max(abs(temp_chan)) > Size_area / 2) or (np.linalg.norm(temp_dist) > Dist_TX_RX):\n",
    "        temp_dist = 2 * Dist_TX_RX * (np.random.rand(1, 2) - 0.5)\n",
    "        temp_chan = Cur_loc + temp_dist\n",
    "    return temp_chan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deeb008-21ce-48c0-97d7-1ce9ea7b131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Channel generation function\n",
    "\n",
    "def ch_gen(Size_area, D2D_dist, Num_D2D, Num_Ch, Num_samples, PL_alpha=38., PL_const=34.5):\n",
    "    \"\"\"\n",
    "    Generate sample data for channels.\n",
    "\n",
    "    Parameters:\n",
    "        Size_area (float): Size of the area.\n",
    "        D2D_dist (float): Distance between D2D users.\n",
    "        Num_D2D (int): Number of D2D users.\n",
    "        Num_Ch (int): Number of channels.\n",
    "        Num_samples (int): Number of samples to generate.\n",
    "        PL_alpha (float): Path loss exponent.\n",
    "        PL_const (float): Path loss constant.\n",
    "\n",
    "    Returns:\n",
    "        ch_w_fading (np.ndarray): Channel matrices with fading.\n",
    "        rx_loc_mat (np.ndarray): Receiver locations matrix.\n",
    "        tx_loc_mat (np.ndarray): Transmitter locations matrix.\n",
    "        CUE_loc_mat (np.ndarray): CUE transmitter locations matrix.\n",
    "    \"\"\"\n",
    "    ch_w_fading = []\n",
    "    rx_loc_mat = []\n",
    "    tx_loc_mat = []\n",
    "    CUE_loc_mat = []\n",
    "\n",
    "    for i in range(Num_samples):\n",
    "        rx_loc, tx_loc, tx_loc_CUE = loc_init(Size_area, D2D_dist, Num_D2D, Num_Ch)\n",
    "        \n",
    "        ch_w_temp_band = []\n",
    "        for j in range(Num_Ch):\n",
    "            tx_loc_with_CUE = np.vstack((tx_loc, tx_loc_CUE[j]))\n",
    "            # Generate distance_vector\n",
    "            dist_vec = np.linalg.norm(rx_loc.reshape(Num_D2D + 1, 1, 2) - tx_loc_with_CUE, axis=2)\n",
    "            dist_vec = np.maximum(dist_vec, 3)\n",
    "\n",
    "            # Calculate path loss (shadowing not considered)\n",
    "            pu_ch_gain_db = -PL_const - PL_alpha * np.log10(dist_vec)\n",
    "            pu_ch_gain = 10 ** (pu_ch_gain_db / 10)\n",
    "\n",
    "            # Multi-fading\n",
    "            multi_fading = 0.5 * np.random.randn(Num_D2D + 1, Num_D2D + 1) ** 2 + \\\n",
    "                           0.5 * np.random.randn(Num_D2D + 1, Num_D2D + 1) ** 2\n",
    "            final_ch = np.maximum(pu_ch_gain * multi_fading, np.exp(-30))\n",
    "            ch_w_temp_band.append(np.transpose(final_ch))\n",
    "\n",
    "        ch_w_fading.append(ch_w_temp_band)\n",
    "        rx_loc_mat.append(rx_loc)\n",
    "        tx_loc_mat.append(tx_loc)\n",
    "        CUE_loc_mat.append(tx_loc_CUE)\n",
    "\n",
    "    return (np.array(ch_w_fading), \n",
    "            np.array(rx_loc_mat), \n",
    "            np.array(tx_loc_mat), \n",
    "            np.array(CUE_loc_mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a44f60-0d54-4a93-9e83-be934a746377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Rate calculation functions\n",
    "\n",
    "def cal_RATE_one_sample_one_channel(channel, tx_power, noise):\n",
    "    \"\"\"\n",
    "    Calculate data rate for a single channel and single sample.\n",
    "\n",
    "    Parameters:\n",
    "        channel (np.ndarray): Channel matrix.\n",
    "        tx_power (np.ndarray): Transmit power.\n",
    "        noise (float): Noise power.\n",
    "\n",
    "    Returns:\n",
    "        cap_val (np.ndarray): Capacity values.\n",
    "    \"\"\"\n",
    "    diag_ch = np.diag(channel)\n",
    "    inter_ch = channel - np.diag(diag_ch)\n",
    "    tot_ch = np.multiply(channel, np.expand_dims(tx_power, -1))\n",
    "    int_ch = np.multiply(inter_ch, np.expand_dims(tx_power, -1))\n",
    "    sig_ch = np.sum(tot_ch - int_ch, axis=1)\n",
    "    int_ch = np.sum(int_ch, axis=1)\n",
    "    SINR_val = np.divide(sig_ch, int_ch + noise)\n",
    "    cap_val = np.log2(1.0 + SINR_val)\n",
    "    return cap_val\n",
    "\n",
    "\n",
    "def cal_CUE_INTER_one_sample_one_channel(channel, tx_power):\n",
    "    \"\"\"\n",
    "    Calculate interference for CUE.\n",
    "\n",
    "    Parameters:\n",
    "        channel (np.ndarray): Channel matrix.\n",
    "        tx_power (np.ndarray): Transmit power.\n",
    "\n",
    "    Returns:\n",
    "        int_ch (np.ndarray): Interference values.\n",
    "    \"\"\"\n",
    "    diag_ch = np.diag(channel)\n",
    "    inter_ch = channel - np.diag(diag_ch)\n",
    "    int_ch = np.multiply(inter_ch, np.expand_dims(tx_power, -1))\n",
    "    int_ch = np.sum(int_ch, axis=1)\n",
    "    return int_ch\n",
    "\n",
    "\n",
    "def cal_rate_NP(channel, tx_power_in, tx_max, noise, DUE_thr, I_thr, P_c):\n",
    "    \"\"\"\n",
    "    Calculate the total spectral efficiency (SE) and energy efficiency (EE).\n",
    "\n",
    "    Parameters:\n",
    "        channel (np.ndarray): Channel matrices.\n",
    "        tx_power_in (np.ndarray): Input transmit power.\n",
    "        tx_max (float): Maximum transmit power.\n",
    "        noise (float): Noise power.\n",
    "        DUE_thr (float): DUE threshold.\n",
    "        I_thr (float): Interference threshold.\n",
    "        P_c (float): Constant power.\n",
    "\n",
    "    Returns:\n",
    "        tot_SE (float): Total spectral efficiency.\n",
    "        tot_EE (float): Total energy efficiency.\n",
    "        PRO_CUE_vio (float): Probability of CUE violation.\n",
    "        PRO_DUE_vio (float): Probability of DUE violation.\n",
    "    \"\"\"\n",
    "    #print(\"channel shape:\", channel.shape)\n",
    "    #print(\"tx_power_in shape:\", tx_power_in.shape)\n",
    "\n",
    "    num_sample = channel.shape[0]\n",
    "    num_channel = channel.shape[1]\n",
    "    num_D2D_user = channel.shape[2] - 1\n",
    "\n",
    "    tot_SE, tot_EE = 0, 0\n",
    "    DUE_violation, CUE_violation = 0, 0\n",
    "\n",
    "    # Ensure tx_power_in has the correct shape\n",
    "    if tx_power_in.shape[0] != num_sample:\n",
    "        tx_power_in = tx_power_in[:num_sample]\n",
    "\n",
    "    # Append tx power and CUE tx power\n",
    "    tx_power = np.concatenate((tx_power_in, np.zeros((tx_power_in.shape[0], 1, num_channel))), axis=1)\n",
    "    \n",
    "    for i in range(num_sample):\n",
    "        cur_cap = 0 \n",
    "        DUE_mask, CUE_mask = 1, 1\n",
    "\n",
    "        for j in range(num_channel):\n",
    "            cur_ch = channel[i][j]\n",
    "            cur_power = tx_power[i, :, j]\n",
    "            cur_ch_cap = cal_RATE_one_sample_one_channel(cur_ch, cur_power, noise)\n",
    "            inter = cal_CUE_INTER_one_sample_one_channel(cur_ch, cur_power)\n",
    "            #print(\"inter shape:\", inter.shape)\n",
    "            #print(\"inter:\", inter)\n",
    "            cur_cap += np.sum(cur_ch_cap[:-1])  # Sum all D2D user capacities\n",
    "            CUE_mask *= (inter[-1] <= I_thr)\n",
    "\n",
    "        # Check if the total D2D capacity meets the threshold\n",
    "        DUE_mask = (cur_cap >= DUE_thr * num_D2D_user)\n",
    "\n",
    "        D2D_SE_sum = cur_cap * CUE_mask * DUE_mask\n",
    "        D2D_EE_sum = cur_cap / (np.sum(tx_power_in[i]) + P_c) * CUE_mask * DUE_mask\n",
    "\n",
    "        if CUE_mask == 0:\n",
    "            CUE_violation += 1\n",
    "\n",
    "        if DUE_mask == 0:\n",
    "            DUE_violation += 1\n",
    "\n",
    "        tot_SE += D2D_SE_sum\n",
    "        tot_EE += D2D_EE_sum\n",
    "\n",
    "    tot_SE /= num_D2D_user * num_sample\n",
    "    tot_EE /= num_D2D_user * num_sample\n",
    "    PRO_DUE_vio = DUE_violation / num_sample\n",
    "    PRO_CUE_vio = CUE_violation / num_sample\n",
    "\n",
    "    return tot_SE, tot_EE, PRO_CUE_vio, PRO_DUE_vio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc532ae7-da72-4b13-8d84-f81d1926a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Power calculation functions\n",
    "\n",
    "def all_possible_tx_power(num_channel, num_user, granularity):\n",
    "    \"\"\"\n",
    "    Generate all possible transmit power combinations.\n",
    "\n",
    "    Parameters:\n",
    "        num_channel (int): Number of channels.\n",
    "        num_user (int): Number of users.\n",
    "        granularity (int): Granularity of power levels.\n",
    "\n",
    "    Returns:\n",
    "        power_mat (np.ndarray): Matrix of possible power levels.\n",
    "    \"\"\"\n",
    "    items = [np.arange(granularity)] * num_user * num_channel\n",
    "    temp_power = list(itertools.product(*items))\n",
    "    temp_power = np.reshape(temp_power, (-1, num_user, num_channel))\n",
    "\n",
    "    power_check = np.sum(temp_power, axis=2)\n",
    "    flag = (power_check / (granularity - 1) <= 1).astype(int)\n",
    "    flag = (np.sum(flag, axis=1) == num_user).astype(int)\n",
    "    temp_power_1 = np.reshape(temp_power, (-1, num_user * num_channel))\n",
    "    temp_power = temp_power_1 * flag[:, np.newaxis]\n",
    "    power = np.reshape(temp_power, (-1, num_user, num_channel)) / (granularity - 1)\n",
    "\n",
    "    power_mat = []\n",
    "    for i in range(power.shape[0]):\n",
    "        sum_val = np.sum(power[i])\n",
    "        if sum_val != 0:\n",
    "            power_mat.append(power[i])\n",
    "\n",
    "    return np.array(power_mat)\n",
    "\n",
    "\n",
    "def optimal_power(channel, tx_max, noise, DUE_thr, I_thr, P_c, tx_power_set, opt=\"SE\"):\n",
    "    \"\"\"\n",
    "    Find the optimal power allocation based on the specified optimization criterion.\n",
    "\n",
    "    Parameters:\n",
    "        channel (np.ndarray): Channel matrices.\n",
    "        tx_max (float): Maximum transmit power.\n",
    "        noise (float): Noise power.\n",
    "        DUE_thr (float): DUE threshold.\n",
    "        I_thr (float): Interference threshold.\n",
    "        P_c (float): Constant power.\n",
    "        tx_power_set (np.ndarray): Set of possible transmit power levels.\n",
    "        opt (str): Optimization criterion (\"SE\" or \"EE\").\n",
    "\n",
    "    Returns:\n",
    "        tot_SE (float): Total spectral efficiency.\n",
    "        tot_EE (float): Total energy efficiency.\n",
    "        PRO_CUE_vio (float): Probability of CUE violation.\n",
    "        PRO_DUE_vio (float): Probability of DUE violation.\n",
    "        chan_infea_mat (np.ndarray): Matrix of infeasible channels.\n",
    "    \"\"\"\n",
    "    num_channel = channel.shape[1]\n",
    "    num_D2D_user = channel.shape[2] - 1\n",
    "    num_samples = channel.shape[0]\n",
    "    tot_SE, tot_EE = 0, 0\n",
    "    power_mat_SE = []\n",
    "    chan_infea_mat = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        cur_cap, DUE_mask, CUE_mask = 0, 1, 1\n",
    "        #tx_power_set = np.expand_dims(tx_power_set, -1)\n",
    "        #tx_power = tx_max * np.hstack((tx_power_set, 0 * np.ones((tx_power_set.shape[0], 1, num_channel))))\n",
    "\n",
    "        # Print the shapes to debug the broadcasting error\n",
    "        #print(\"Shape of tx_power_set:\", tx_power_set.shape)\n",
    "        #print(\"Shape of ones array:\", np.ones((tx_power_set.shape[0], 1, num_channel)).shape)\n",
    "\n",
    "        # Adjust tx_power_set to make it compatible for concatenation\n",
    "        #tx_power_set_reshaped = np.reshape(tx_power_set, (tx_power_set.shape[0], -1, num_channel))\n",
    "\n",
    "        # Ensure dimensions match before concatenation\n",
    "        #tx_power = tx_max * np.hstack((tx_power_set_reshaped, np.zeros((tx_power_set.shape[0], 1, num_channel))))\n",
    "        \n",
    "        tx_power_set_reshaped = tx_power_set.reshape(-1, num_D2D_user, num_channel)\n",
    "        tx_power = tx_max * np.concatenate((\n",
    "            tx_power_set_reshaped,\n",
    "            np.zeros((tx_power_set_reshaped.shape[0], 1, num_channel))\n",
    "        ), axis=1)\n",
    "        \n",
    "\n",
    "        for j in range(num_channel):\n",
    "            cur_ch = channel[i][j]\n",
    "            cur_ch_cap = cal_RATE_one_sample_one_channel(cur_ch, tx_power[:, :, j], noise)\n",
    "            inter = cal_CUE_INTER_one_sample_one_channel(cur_ch, tx_power[:, :, j])\n",
    "            cur_cap += cur_ch_cap\n",
    "            CUE_mask *= (inter[:, num_D2D_user] < I_thr)\n",
    "\n",
    "        for j in range(num_D2D_user):\n",
    "            DUE_mask *= (cur_cap[:, j] > DUE_thr)\n",
    "\n",
    "        CUE_mask = np.expand_dims(CUE_mask, -1)\n",
    "        DUE_mask = np.expand_dims(DUE_mask, -1)\n",
    "\n",
    "        sum_D2D_SE_temp = np.expand_dims(np.sum(cur_cap[:, :-1], axis=1), -1)\n",
    "        sum_D2D_EE_temp = np.expand_dims(\n",
    "            np.sum(cur_cap[:, :-1] / (np.sum(tx_power[:, :-1, :], axis=2) + P_c), axis=1), -1)\n",
    "\n",
    "        D2D_SE_sum = sum_D2D_SE_temp * DUE_mask\n",
    "        D2D_EE_sum = sum_D2D_EE_temp * DUE_mask\n",
    "\n",
    "        if opt == \"SE\":\n",
    "            arg_max_val = np.argmax(D2D_SE_sum)\n",
    "        else:\n",
    "            arg_max_val = np.argmax(D2D_EE_sum)\n",
    "\n",
    "        max_SE = np.max(D2D_SE_sum)\n",
    "\n",
    "        found_tx_val = tx_power[arg_max_val][:-1]\n",
    "        power_mat_SE.append(found_tx_val)\n",
    "\n",
    "        # Collect the infeasible channels\n",
    "        if max_SE == 0:\n",
    "            chan_infea_mat.append(channel[i])\n",
    "\n",
    "    power_mat_SE = np.array(power_mat_SE)\n",
    "    tot_SE, tot_EE, PRO_CUE_vio, PRO_DUE_vio = cal_rate_NP(\n",
    "        channel, power_mat_SE, tx_max, noise, DUE_thr, I_thr, P_c)\n",
    "\n",
    "    return tot_SE, tot_EE, PRO_CUE_vio, PRO_DUE_vio, np.array(chan_infea_mat)\n",
    "\n",
    "\n",
    "def optimal_power_w_chan(channel, tx_max, noise, DUE_thr, I_thr, P_c, tx_power_set, opt=\"SE\"):\n",
    "    \"\"\"\n",
    "    Optimal power allocation with channel considerations.\n",
    "\n",
    "    Parameters:\n",
    "        channel (np.ndarray): Channel matrices.\n",
    "        tx_max (float): Maximum transmit power.\n",
    "        noise (float): Noise power.\n",
    "        DUE_thr (float): DUE threshold.\n",
    "        I_thr (float): Interference threshold.\n",
    "        P_c (float): Constant power.\n",
    "        tx_power_set (np.ndarray): Set of possible transmit power levels.\n",
    "        opt (str): Optimization criterion (\"SE\" or \"EE\").\n",
    "\n",
    "    Returns:\n",
    "        tot_SE (float): Total spectral efficiency.\n",
    "        tot_EE (float): Total energy efficiency.\n",
    "        PRO_CUE_vio (float): Probability of CUE violation.\n",
    "        PRO_DUE_vio (float): Probability of DUE violation.\n",
    "        chan_infea_mat (np.ndarray): Matrix of infeasible channels.\n",
    "        power_mat_SE (np.ndarray): Power matrix for SE.\n",
    "        channel (np.ndarray): Channel matrices.\n",
    "    \"\"\"\n",
    "    num_channel = channel.shape[1] if channel.ndim > 1 else 1\n",
    "    num_D2D_user = channel.shape[2] - 1 if channel.ndim > 2 else channel.shape[0] - 1\n",
    "    num_samples = channel.shape[0] if channel.ndim > 2 else 1\n",
    "    \n",
    "    #num_channel = channel.shape[1]\n",
    "    #num_D2D_user = channel.shape[2] - 1\n",
    "    #num_samples = channel.shape[0]\n",
    "    #num_samples, num_channel, num_D2D_user, _ = channel.shape\n",
    "    tot_SE, tot_EE = 0, 0\n",
    "    power_mat_SE = []\n",
    "    chan_infea_mat = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        cur_cap, DUE_mask, CUE_mask = 0, 1, 1\n",
    "        # Reshape tx_power_set to match the expected dimensions\n",
    "        tx_power_set_reshaped = tx_power_set.reshape(-1, num_D2D_user, num_channel)\n",
    "        \n",
    "        #tx_power_set = np.expand_dims(tx_power_set, -1)\n",
    "        \n",
    "        #tx_power = tx_max * np.hstack((tx_power_set, 0 * np.ones((tx_power_set.shape[0], 1, num_channel))))\n",
    "        \n",
    "        # Concatenate with zeros for CUE\n",
    "        tx_power = tx_max * np.concatenate((\n",
    "            tx_power_set_reshaped,\n",
    "            np.zeros((tx_power_set_reshaped.shape[0], 1, num_channel))\n",
    "        ), axis=1)\n",
    "\n",
    "        for j in range(num_channel):\n",
    "            cur_ch = channel[i][j]\n",
    "            cur_ch_cap = cal_RATE_one_sample_one_channel(cur_ch, tx_power[:, :, j], noise)\n",
    "            inter = cal_CUE_INTER_one_sample_one_channel(cur_ch, tx_power[:, :, j])\n",
    "            cur_cap += cur_ch_cap\n",
    "            CUE_mask *= (inter[:, num_D2D_user] < I_thr)\n",
    "\n",
    "        for j in range(num_D2D_user):\n",
    "            DUE_mask *= (cur_cap[:, j] > DUE_thr)\n",
    "\n",
    "        CUE_mask = np.expand_dims(CUE_mask, -1)\n",
    "        DUE_mask = np.expand_dims(DUE_mask, -1)\n",
    "\n",
    "        sum_D2D_SE_temp = np.expand_dims(np.sum(cur_cap[:, :-1], axis=1), -1)\n",
    "        sum_D2D_EE_temp = np.expand_dims(\n",
    "            np.sum(cur_cap[:, :-1] / (np.sum(tx_power[:, :-1, :], axis=2) + P_c), axis=1), -1)\n",
    "\n",
    "        D2D_SE_sum = sum_D2D_SE_temp\n",
    "        D2D_EE_sum = sum_D2D_EE_temp\n",
    "\n",
    "        if opt == \"SE\":\n",
    "            arg_max_val = np.argmax(D2D_SE_sum)\n",
    "        else:\n",
    "            arg_max_val = np.argmax(D2D_EE_sum)\n",
    "\n",
    "        max_SE = np.max(D2D_SE_sum)\n",
    "\n",
    "        found_tx_val = tx_power[arg_max_val][:-1]\n",
    "        power_mat_SE.append(found_tx_val)\n",
    "\n",
    "        # Collect the infeasible channels\n",
    "        if max_SE == 0:\n",
    "            chan_infea_mat.append(channel[i])\n",
    "\n",
    "    power_mat_SE = np.array(power_mat_SE)\n",
    "    #print(\"channel shape:\", channel.shape)\n",
    "    #print(\"tx_power_in shape:\", tx_power_in.shape)\n",
    "    tot_SE, tot_EE, PRO_CUE_vio, PRO_DUE_vio = cal_rate_NP(\n",
    "        channel, power_mat_SE, tx_max, noise, DUE_thr, I_thr, P_c)\n",
    "\n",
    "    return tot_SE, tot_EE, PRO_CUE_vio, PRO_DUE_vio, np.array(chan_infea_mat), power_mat_SE, channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9d473f-4023-4b8a-a73c-a53b0e2e66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Spectral and Energy Efficiency Calculation\n",
    "\n",
    "def cal_SE_EE(channel, tx_max, noise, DUE_thr, I_thr, P_c, tx_power_mat, opt=\"SE\"):\n",
    "    \"\"\"\n",
    "    Calculate Spectral Efficiency (SE) and Energy Efficiency (EE).\n",
    "\n",
    "    Parameters:\n",
    "        channel (np.ndarray): Channel matrix.\n",
    "        tx_max (float): Maximum transmit power.\n",
    "        noise (float): Noise power.\n",
    "        DUE_thr (float): DUE threshold.\n",
    "        I_thr (float): Interference threshold.\n",
    "        P_c (float): Constant power.\n",
    "        tx_power_mat (np.ndarray): Transmit power matrix.\n",
    "        opt (str): Optimization criterion (\"SE\" or \"EE\").\n",
    "\n",
    "    Returns:\n",
    "        D2D_SE_sum (float): Sum of SE for D2D users.\n",
    "        D2D_EE_sum (float): Sum of EE for D2D users.\n",
    "    \"\"\"\n",
    "    num_D2D_user = channel.shape[0] - 1\n",
    "\n",
    "    cur_cap, DUE_mask, CUE_mask = 0, 1, 1\n",
    "\n",
    "    # Append CUE power as zero\n",
    "    tx_power = np.vstack((tx_power_mat, 0 * np.ones((1, 1))))\n",
    "    tx_power = np.expand_dims(tx_power, 0)\n",
    "\n",
    "    cur_ch = channel\n",
    "    cur_ch_cap = cal_RATE_one_sample_one_channel(cur_ch, tx_power[:, :, 0], noise)\n",
    "    cur_cap += cur_ch_cap\n",
    "\n",
    "    sum_D2D_SE_temp = np.sum(cur_cap[0, :-1])\n",
    "    sum_D2D_EE_temp = np.sum(cur_cap[0, :-1] / (tx_power[0, :-1, 0] + P_c))\n",
    "\n",
    "    D2D_SE_sum = sum_D2D_SE_temp\n",
    "    D2D_EE_sum = sum_D2D_EE_temp\n",
    "\n",
    "    return D2D_SE_sum, D2D_EE_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64abbc3-6f05-4459-a15e-68270b68440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Initialize simulation parameters\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Power level settings\n",
    "Num_power_level = 100\n",
    "\n",
    "# Initial simulation settings\n",
    "Size_area = 50.0\n",
    "D2D_dist = 10\n",
    "Num_user = 2\n",
    "Num_channel = 1\n",
    "num_samples_tr = 30\n",
    "\n",
    "# Generate initial channel data\n",
    "ch_mat, rx_mat, tx_mat, CUE_mat = ch_gen(Size_area, D2D_dist, Num_user, Num_channel, int(10**4))\n",
    "tx_power_in = 10**2.0 * np.ones((ch_mat.shape[0], 2, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64035f7e-8c51-4f21-9726-67eb7b893200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /opt/app-root/src/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A30, compute capability 8.0, VMM: yes\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 578 tensors from /opt/app-root/src/.cache/huggingface/hub/models--ibm-granite--granite-8b-code-instruct-4k-GGUF/snapshots/189b3d81167eccf74d8afb4b4f5da1f1d0a3f7a0/granite-8b-code-instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = granite-8b-code-instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 36\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  14:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = refact\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<fim_prefix>\", \"<f...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  325 tensors\n",
      "llama_model_loader: - type q4_K:  216 tensors\n",
      "llama_model_loader: - type q6_K:   37 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 19/49152 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48891\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 36\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.05 B\n",
      "llm_load_print_meta: model size       = 4.55 GiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name     = granite-8b-code-instruct\n",
      "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 145 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.44 MiB\n",
      "llama_model_load: error loading model: create_tensor: tensor 'output.weight' not found\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: /opt/app-root/src/.cache/huggingface/hub/models--ibm-granite--granite-8b-code-instruct-4k-GGUF/snapshots/189b3d81167eccf74d8afb4b4f5da1f1d0a3f7a0/granite-8b-code-instruct.Q4_K_M.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 108\u001b[0m\n\u001b[1;32m    102\u001b[0m llm_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgranite-8b-code-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# model_path = sys.argv[1]  # This would typically be passed as a command-line argument\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# For notebook purposes, specify the model path directly\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# model_path = \"./models/codellama-7b.Q5_K_M.gguf\"  # Update this path accordingly\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36minitialize_llm\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_llm\u001b[39m(model_path):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Initialize the Llama model.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m        llm (Llama): Initialized Llama model.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use GPU acceleration\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m26200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/llama_cpp/llama.py:311\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_ \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/llama_cpp/_internals.py:55\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[0;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to load model from file: /opt/app-root/src/.cache/huggingface/hub/models--ibm-granite--granite-8b-code-instruct-4k-GGUF/snapshots/189b3d81167eccf74d8afb4b4f5da1f1d0a3f7a0/granite-8b-code-instruct.Q4_K_M.gguf"
     ]
    }
   ],
   "source": [
    "# Cell 8: Llama model setup and initial query\n",
    "\n",
    "import sys\n",
    "\n",
    "def initialize_llm(model_path):\n",
    "    \"\"\"\n",
    "    Initialize the Llama model.\n",
    "\n",
    "    Parameters:\n",
    "        model_path (str): Path to the Llama model.\n",
    "\n",
    "    Returns:\n",
    "        llm (Llama): Initialized Llama model.\n",
    "    \"\"\"\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=-1,  # Use GPU acceleration\n",
    "        n_ctx=26200,\n",
    "        verbose=True\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def prepare_initial_query():\n",
    "    \"\"\"\n",
    "    Prepare the initial query text for the Llama model.\n",
    "\n",
    "    Returns:\n",
    "        query_text (str): The query string.\n",
    "    \"\"\"\n",
    "    query_text = \"\"\"\n",
    "    Take a deep breath and work on this problem step-by-step. You are a mathematical tool to predict some model. Your job is to predict B for given A. The following is the dataset that you can use for the prediction.\n",
    "    If A is 59.6, 73.2, 59.8, 63.6, then B is 100, 100.\n",
    "    If A is 71.0, 59.8, 61.9, 73.7, then B is 0, 100.\n",
    "    If A is 61.4, 65.8, 66.0, 66.9, then B is 100, 0.\n",
    "    If A is 62.3, 58.9, 72.8, 54.0, then B is 100, 100.\n",
    "    If A is 48.6, 55.0, 52.0, 48.9, then B is 100, 100.\n",
    "    If A is 74.3, 57.9, 76.7, 62.9, then B is 0, 100.\n",
    "    If A is 53.4, 51.3, 61.1, 68.9, then B is 100, 0.\n",
    "    If A is 83.0, 55.9, 68.0, 56.6, then B is 0, 100.\n",
    "    If A is 60.6, 65.0, 66.7, 58.6, then B is 100, 100.\n",
    "    If A is 72.1, 69.6, 58.7, 54.3, then B is 0, 100.\n",
    "    If A is 58.7, 71.6, 72.1, 50.1, then B is 100, 100.\n",
    "    If A is 63.1, 68.3, 84.8, 63.8, then B is 100, 100.\n",
    "    If A is 74.1, 62.3, 64.0, 68.9, then B is 0, 100.\n",
    "    If A is 70.1, 75.8, 50.0, 70.0, then B is 100, 0.\n",
    "    If A is 60.3, 62.0, 64.2, 74.0, then B is 100, 0.\n",
    "    If A is 52.1, 64.6, 57.4, 52.2, then B is 0, 100.\n",
    "    If A is 55.7, 59.2, 62.2, 54.0, then B is 0, 100.\n",
    "    If A is 49.4, 63.1, 50.3, 62.9, then B is 100, 0.\n",
    "    If A is 52.1, 71.5, 58.6, 60.1, then B is 100, 100.\n",
    "    If A is 71.1, 54.9, 51.3, 57.8, then B is 0, 100.\n",
    "    If A is 66.0, 60.1, 73.5, 69.6, then B is 100, 100.\n",
    "    If A is 58.7, 48.0, 63.0, 53.3, then B is 0, 100.\n",
    "    If A is 71.5, 64.3, 65.2, 67.5, then B is 0, 100.\n",
    "    If A is 67.3, 67.0, 78.9, 67.3, then B is 0, 100.\n",
    "    If A is 62.1, 59.9, 54.4, 75.7, then B is 100, 0.\n",
    "    If A is 76.7, 50.1, 82.5, 52.4, then B is 0, 100.\n",
    "    If A is 58.8, 66.3, 64.6, 68.1, then B is 100, 0.\n",
    "    If A is 64.6, 61.3, 49.5, 55.8, then B is 100, 0.\n",
    "    If A is 55.4, 55.7, 54.4, 55.5, then B is 100, 0.\n",
    "    If A is 67.2, 70.0, 73.2, 76.8, then B is 100, 100.\n",
    "    If A is 58.4, 59.6, 58.5, 50.8, then B is 100, 0.\n",
    "    If A is 68.7, 48.6, 67.5, 63.0, then B is 0, 100.\n",
    "    If A is 75.0, 57.6, 50.3, 65.5, then B is 0, 100.\n",
    "    If A is 52.6, 60.2, 69.6, 55.6, then B is 100, 0.\n",
    "    If A is 64.3, 69.1, 70.2, 77.7, then B is 100, 100.\n",
    "    If A is 51.9, 62.6, 111.9, 68.7, then B is 100, 100.\n",
    "    If A is 61.2, 79.7, 58.8, 66.1, then B is 100, 0.\n",
    "    If A is 84.6, 51.6, 62.6, 54.9, then B is 0, 100.\n",
    "    If A is 54.2, 53.2, 49.7, 53.7, then B is 100, 0.\n",
    "    If A is 55.9, 50.2, 70.5, 85.1, then B is 100, 0.\n",
    "    If A is 69.2, 67.5, 52.5, 71.9, then B is 100, 100.\n",
    "    If A is 58.6, 66.6, 49.8, 65.4, then B is 100, 0.\n",
    "    If A is 77.2, 75.1, 74.2, 69.9, then B is 100, 100.\n",
    "    If A is 79.5, 59.9, 67.1, 63.3, then B is 0, 100.\n",
    "    If A is 61.7, 63.5, 66.6, 82.9, then B is 100, 0.\n",
    "    If A is 66.6, 64.2, 67.7, 67.3, then B is 100, 0.\n",
    "    If A is 73.7, 57.0, 65.8, 54.3, then B is 0, 100.\n",
    "    If A is 56.2, 62.9, 50.8, 66.1, then B is 100, 0.\n",
    "    If A is 57.9, 57.3, 53.2, 47.5, then B is 0, 100.\n",
    "    If A is 64.3, 67.8, 60.9, 55.0, then B is 100, 0.\n",
    "    ...\n",
    "    If A is 52.4, 67.5, 57.1, 55.3, then B is \n",
    "    \"\"\"\n",
    "    return query_text\n",
    "\n",
    "\n",
    "# Install the required libraries (run this if needed)\n",
    "# !pip install transformers huggingface_hub\n",
    "\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "import shutil\n",
    "\n",
    "# Step 1: Authenticate with Hugging Face\n",
    "login(token=\"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\")  # Replace with your token\n",
    "\n",
    "#Step 2: Download the model\n",
    "model_name_or_path = \"ibm-granite/granite-8b-code-instruct-4k-GGUF\"\n",
    "model_basename = \"granite-8b-code-instruct.Q4_K_M.gguf\"\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "\n",
    "llm_name = \"granite-8b-code-instruct\"\n",
    "\n",
    "# Example usage:\n",
    "# model_path = sys.argv[1]  # This would typically be passed as a command-line argument\n",
    "# For notebook purposes, specify the model path directly\n",
    "# model_path = \"./models/codellama-7b.Q5_K_M.gguf\"  # Update this path accordingly\n",
    "llm = initialize_llm(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba270c-d13d-40fb-af34-bf8637cfb5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Generate power levels and define optimization parameters\n",
    "\n",
    "# Generate all possible transmit power combinations\n",
    "tx_power_set = all_possible_tx_power(Num_channel, Num_user, Num_power_level - 1)\n",
    "\n",
    "# Original simulation settings\n",
    "Size_area_original = 20\n",
    "D2D_dist_original = 15\n",
    "tx_max_original = 10**2.0\n",
    "\n",
    "# Thresholds and constants\n",
    "DUE_thr = 4.0\n",
    "I_thr = 10**(-55.0/10)\n",
    "P_c = 2 * 10**2.0\n",
    "BW = 1e7\n",
    "noise = BW * 10**-17.4\n",
    "\n",
    "# Update simulation settings as per original script\n",
    "Size_area = Size_area_original\n",
    "D2D_dist = D2D_dist_original\n",
    "tx_max = tx_max_original\n",
    "\n",
    "# Generate channel data for further simulations\n",
    "Num_sample = 10\n",
    "ch_mat, rx_mat, tx_mat, CUE_mat = ch_gen(Size_area, D2D_dist, Num_user, Num_channel, Num_sample)\n",
    "ch_mat_log = np.log(ch_mat)\n",
    "chan_avg = np.mean(ch_mat_log)\n",
    "chan_std = np.std(ch_mat_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b033d4d-58bf-49d3-8862-83322672392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Execute initial Llama query\n",
    "\n",
    "# Prepare the initial query\n",
    "initial_query_text = prepare_initial_query()\n",
    "\n",
    "# Execute the query and print the result\n",
    "initial_llm_result = llm(initial_query_text, stop=[\".\"])[\"choices\"][0][\"text\"]\n",
    "print(\"Initial Llama Query Result:\")\n",
    "print(initial_llm_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c826ea-1676-418b-bcc6-94d738526060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Main simulation loop with adjusted print statements\n",
    "\n",
    "def run_simulation(llm, batch_sizes, num_iterations, Size_area, D2D_dist, Num_user, Num_channel, \n",
    "                   tx_max, noise, DUE_thr, I_thr, P_c, tx_power_set, chan_avg, chan_std, critera=\"EE\"):\n",
    "    \"\"\"\n",
    "    Run the main simulation loop with various batch sizes.\n",
    "\n",
    "    Parameters:\n",
    "        llm (Llama): Initialized Llama model.\n",
    "        batch_sizes (list): List of batch sizes to iterate over.\n",
    "        num_iterations (int): Number of iterations per batch size.\n",
    "        Size_area (float): Size of the area.\n",
    "        D2D_dist (float): Distance between D2D users.\n",
    "        Num_user (int): Number of users.\n",
    "        Num_channel (int): Number of channels.\n",
    "        tx_max (float): Maximum transmit power.\n",
    "        noise (float): Noise power.\n",
    "        DUE_thr (float): DUE threshold.\n",
    "        I_thr (float): Interference threshold.\n",
    "        P_c (float): Constant power.\n",
    "        tx_power_set (np.ndarray): Set of possible transmit power levels.\n",
    "        chan_avg (float): Average of log channels.\n",
    "        chan_std (float): Standard deviation of log channels.\n",
    "        critera (str): Optimization criterion (\"SE\" or \"EE\").\n",
    "\n",
    "    Returns:\n",
    "        results (dict): Dictionary containing aggregated results for each batch size.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for i_1, batch_size in enumerate(batch_sizes):\n",
    "        print(f\"Starting simulation for batch_size = {batch_size}\")\n",
    "        SE_opt_mat, EE_opt_mat = 0, 0\n",
    "        SE_prop_mat, EE_prop_mat = 0, 0\n",
    "        SE_prop_2_mat, EE_prop_2_mat = 0, 0\n",
    "        SE_rand_mat, EE_rand_mat = 0, 0\n",
    "        SE_bin_mat, EE_bin_mat = 0, 0\n",
    "\n",
    "        for j in tqdm(range(num_iterations), desc=f\"Batch Size {batch_size}\"):\n",
    "            # Generate channel data\n",
    "            ch_mat_val, rx_mat_val, tx_mat_val, CUE_mat_val = ch_gen(Size_area, D2D_dist, Num_user, Num_channel, 1000)\n",
    "            \n",
    "            # Optimal power allocation\n",
    "            SE_OPT_val, EE_OPT_val, CUE_vio_OPT_val, DUE_vio_OPT, INF_CHAN_MAT_val, PW_VEC_val, CHAN_VEC_val = \\\n",
    "                optimal_power_w_chan(ch_mat_val, tx_max, noise, DUE_thr, I_thr, P_c, tx_power_set, opt=critera)\n",
    "            \n",
    "            # Prepare query text\n",
    "            query_text = 'Take a deep breath and work on this problem step-by-step. You are a mathematical tool to predict some model. Your job is to predict B for given A. The following is the dataset that you can use for the prediction.\\n'\n",
    "            \n",
    "            for i in range(PW_VEC_val.shape[0]):\n",
    "                chan_revised = (np.log(ch_mat_val[i, 0, :, :]) - chan_avg) / chan_std * 100\n",
    "\n",
    "                if i == PW_VEC_val.shape[0]-1:\n",
    "                    chan_revised_val = (np.log(ch_mat_val[j, 0, :, :]) - chan_avg) / chan_std * 100\n",
    "                    query_text += f'If A is {chan_revised_val[0, 0]:0.0f}, {chan_revised_val[0, 1]:0.0f}, {chan_revised_val[1, 0]:0.0f}, {chan_revised_val[1, 1]:0.0f}, then B is '\n",
    "                    print(f'[TRUE VALUE] If A is {chan_revised[0, 0]:0.2f}, {chan_revised[0, 1]:0.0f}, {chan_revised[1, 0]:0.0f}, {chan_revised[1, 1]:0.0f}, then B is ')\n",
    "                    print(f'[TRUE VALUE] B is {PW_VEC_val[i, 0, 0]:0.0f}, {PW_VEC_val[i, 1, 0]:0.0f}')\n",
    "\n",
    "                    SE_opt, EE_opt = cal_SE_EE(\n",
    "                        ch_mat_val[i, 0, :, :], tx_max, noise, DUE_thr, I_thr, P_c, PW_VEC_val[j], opt=critera)\n",
    "                    print(\"SE_opt = \", SE_opt, \"EE_opt = \", EE_opt*1000)\n",
    "\n",
    "                if i < batch_size:\n",
    "                    query_text += f'If A is {chan_revised[0, 0]:0.0f}, {chan_revised[0, 1]:0.0f}, {chan_revised[1, 0]:0.0f}, {chan_revised[1, 1]:0.0f}, then B is {PW_VEC_val[i, 0, 0]:0.0f}, {PW_VEC_val[i, 1, 0]:0.0f}.\\n'\n",
    "            \n",
    "            # Execute Llama query\n",
    "            llm_result = llm(query_text, stop=[\".\"])[\"choices\"][0][\"text\"]\n",
    "            print(\"Query Text:\")\n",
    "            print(query_text)\n",
    "            print(\"LLM Result:\")\n",
    "            print(llm_result)\n",
    "\n",
    "            # Process LLM result\n",
    "            SE_prop, EE_prop = 0, 0\n",
    "            temp_dict = llm_result.split(\",\")\n",
    "            if len(temp_dict) == 2:\n",
    "                try:\n",
    "                    temp_PW = np.expand_dims(np.asarray(temp_dict).astype(float), -1)\n",
    "                except ValueError:\n",
    "                    temp_PW = 0 * np.random.rand(2, 1)\n",
    "                SE_prop, EE_prop = cal_SE_EE(\n",
    "                    ch_mat_val[j, 0, :, :], tx_max, noise, DUE_thr, I_thr, P_c, temp_PW, opt=critera)\n",
    "                print(\"SE_prop = \", SE_prop, \"EE_prop = \", EE_prop * 1000)\n",
    "\n",
    "            # Random power allocation\n",
    "            temp_PW_rand = tx_max * np.random.rand(2, 1)\n",
    "            print(\"Random Power Allocation:\")\n",
    "            print(temp_PW_rand)\n",
    "            SE_rand, EE_rand = cal_SE_EE(\n",
    "                ch_mat_val[j, 0, :, :], tx_max, noise, DUE_thr, I_thr, P_c, temp_PW_rand, opt=critera)\n",
    "            print(\"SE_rand = \", SE_rand, \"EE_rand = \", EE_rand * 1000)\n",
    "            print(\"**\" * 50)\n",
    "\n",
    "            # Binary power allocation\n",
    "            temp_val = np.random.rand()\n",
    "            if temp_val < 0.5:\n",
    "                temp_PW_rand[0, 0] = 100\n",
    "                temp_PW_rand[1, 0] = 0\n",
    "            else:\n",
    "                temp_PW_rand[1, 0] = 100\n",
    "                temp_PW_rand[0, 0] = 0\n",
    "\n",
    "            print(\"Binary Power Allocation:\")\n",
    "            print(temp_PW_rand)\n",
    "            SE_bin, EE_bin = cal_SE_EE(\n",
    "                ch_mat_val[j, 0, :, :], tx_max, noise, DUE_thr, I_thr, P_c, temp_PW_rand, opt=\"EE\")\n",
    "            print(\"SE_bin = \", SE_bin, \"EE_bin = \", EE_bin * 1000)\n",
    "            print(\"**\" * 50)\n",
    "\n",
    "            # Optimal power allocation\n",
    "            SE_opt, EE_opt = cal_SE_EE(\n",
    "                ch_mat_val[j, 0, :, :], tx_max, noise, DUE_thr, I_thr, P_c, PW_VEC_val[j], opt=critera)\n",
    "\n",
    "            # Compare and choose the better option based on criterion\n",
    "            if critera == \"SE\":\n",
    "                if SE_bin > SE_prop:\n",
    "                    SE_prop_2, EE_prop_2 = SE_bin, EE_bin\n",
    "                else:\n",
    "                    SE_prop_2, EE_prop_2 = SE_prop, EE_prop\n",
    "            elif critera == \"EE\":\n",
    "                if EE_bin > EE_prop:\n",
    "                    SE_prop_2, EE_prop_2 = SE_bin, EE_bin\n",
    "                else:\n",
    "                    SE_prop_2, EE_prop_2 = SE_prop, EE_prop\n",
    "\n",
    "            # Aggregate results\n",
    "            SE_opt_mat += SE_opt\n",
    "            EE_opt_mat += EE_opt * 1000\n",
    "\n",
    "            SE_prop_mat += SE_prop\n",
    "            EE_prop_mat += EE_prop * 1000\n",
    "\n",
    "            SE_prop_2_mat += SE_prop_2\n",
    "            EE_prop_2_mat += EE_prop_2 * 1000\n",
    "\n",
    "            SE_rand_mat += SE_rand\n",
    "            EE_rand_mat += EE_rand * 1000\n",
    "\n",
    "            SE_bin_mat += SE_bin\n",
    "            EE_bin_mat += EE_bin * 1000\n",
    "\n",
    "            # Periodic progress update\n",
    "            if (j + 1) % 50 == 0:\n",
    "                print(f'Iteration {j+1}: [OPT] SE: {SE_opt_mat/(j+1):0.1f}, EE: {EE_opt_mat/(j+1):0.1f}, '\n",
    "                      f'[PROP] SE: {SE_prop_mat/(j+1):0.1f}, EE: {EE_prop_mat/(j+1):0.1f}, '\n",
    "                      f'[PROP_2] SE: {SE_prop_2_mat/(j+1):0.1f}, EE: {EE_prop_2_mat/(j+1):0.1f}, '\n",
    "                      f'[RAND] SE: {SE_rand_mat/(j+1):0.1f}, EE: {EE_rand_mat/(j+1):0.1f}, '\n",
    "                      f'[BIN] SE: {SE_bin_mat/(j+1):0.1f}, EE: {EE_bin_mat/(j+1):0.1f}')\n",
    "\n",
    "        # Final results for the current batch size\n",
    "        print(\"Final results for batch_size =\", batch_size)\n",
    "        print(f'[OPT] SE: {SE_opt_mat / num_iterations:0.1f}, EE: {EE_opt_mat / num_iterations:0.1f}, '\n",
    "              f'[PROP] SE: {SE_prop_mat / num_iterations:0.1f}, EE: {EE_prop_mat / num_iterations:0.1f}, '\n",
    "              f'[RAND] SE: {SE_rand_mat / num_iterations:0.1f}, EE: {EE_rand_mat / num_iterations:0.1f}')\n",
    "        print(\"*\" * 50)\n",
    "\n",
    "        # Store results\n",
    "        results[batch_size] = {\n",
    "            \"OPT_SE\": SE_opt_mat / num_iterations,\n",
    "            \"OPT_EE\": EE_opt_mat / num_iterations,\n",
    "            \"PROP_SE\": SE_prop_mat / num_iterations,\n",
    "            \"PROP_EE\": EE_prop_mat / num_iterations,\n",
    "            \"RAND_SE\": SE_rand_mat / num_iterations,\n",
    "            \"RAND_EE\": EE_rand_mat / num_iterations,\n",
    "            \"BIN_SE\": SE_bin_mat / num_iterations,\n",
    "            \"BIN_EE\": EE_bin_mat / num_iterations\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define batch sizes and number of iterations\n",
    "batch_sizes = [50 * (2**i) for i in range(5)]  # [25, 50, 100, 200, 400]\n",
    "num_iterations = 100  # Number of iterations per batch size\n",
    "critera = \"EE\"  # Optimization criterion: \"SE\" or \"EE\"\n",
    "\n",
    "# Run the simulation\n",
    "simulation_results = run_simulation(\n",
    "    llm=llm,\n",
    "    batch_sizes=batch_sizes,\n",
    "    num_iterations=num_iterations,\n",
    "    Size_area=Size_area,\n",
    "    D2D_dist=D2D_dist,\n",
    "    Num_user=Num_user,\n",
    "    Num_channel=Num_channel,\n",
    "    tx_max=tx_max,\n",
    "    noise=noise,\n",
    "    DUE_thr=DUE_thr,\n",
    "    I_thr=I_thr,\n",
    "    P_c=P_c,\n",
    "    tx_power_set=tx_power_set,\n",
    "    chan_avg=chan_avg,\n",
    "    chan_std=chan_std,\n",
    "    critera=critera\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056eb3c-f98d-4c94-8678-6f449f3ebe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Displaying final simulation results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def display_results(results):\n",
    "    \"\"\"\n",
    "    Display the simulation results in a tabular format.\n",
    "\n",
    "    Parameters:\n",
    "        results (dict): Dictionary containing aggregated results for each batch size.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    data = []\n",
    "    for batch_size, metrics in results.items():\n",
    "        row = {\"Batch Size\": int(batch_size)}\n",
    "        for metric in ['OPT_SE', 'OPT_EE', 'PROP_SE', 'PROP_EE', 'RAND_SE', 'RAND_EE', 'BIN_SE', 'BIN_EE']:\n",
    "            row[metric] = float(metrics.get(metric, 0))  # Use 0 as default if metric is missing\n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    display(df)\n",
    "\n",
    "# Display the results\n",
    "display_results(simulation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cc565-e921-4d73-ba84-07f693efc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Plotting the results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    Plot SE and EE for different strategies across batch sizes.\n",
    "\n",
    "    Parameters:\n",
    "        results (dict): Dictionary containing aggregated results for each batch size.\n",
    "    \"\"\"\n",
    "    batch_sizes = sorted(results.keys())\n",
    "    OPT_SE = [results[bs][\"OPT_SE\"] for bs in batch_sizes]\n",
    "    OPT_EE = [results[bs][\"OPT_EE\"] for bs in batch_sizes]\n",
    "    PROP_SE = [results[bs][\"PROP_SE\"] for bs in batch_sizes]\n",
    "    PROP_EE = [results[bs][\"PROP_EE\"] for bs in batch_sizes]\n",
    "    RAND_SE = [results[bs][\"RAND_SE\"] for bs in batch_sizes]\n",
    "    RAND_EE = [results[bs][\"RAND_EE\"] for bs in batch_sizes]\n",
    "    BIN_SE = [results[bs][\"BIN_SE\"] for bs in batch_sizes]\n",
    "    BIN_EE = [results[bs][\"BIN_EE\"] for bs in batch_sizes]\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot SE\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(batch_sizes, OPT_SE, label='OPT_SE')\n",
    "    plt.plot(batch_sizes, PROP_SE, label='PROP_SE')\n",
    "    plt.plot(batch_sizes, RAND_SE, label='RAND_SE')\n",
    "    plt.plot(batch_sizes, BIN_SE, label='BIN_SE')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Spectral Efficiency (SE)')\n",
    "    plt.title('Spectral Efficiency vs Batch Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot EE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(batch_sizes, OPT_EE, label='OPT_EE')\n",
    "    plt.plot(batch_sizes, PROP_EE, label='PROP_EE')\n",
    "    plt.plot(batch_sizes, RAND_EE, label='RAND_EE')\n",
    "    plt.plot(batch_sizes, BIN_EE, label='BIN_EE')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Energy Efficiency (EE)')\n",
    "    plt.title('Energy Efficiency vs Batch Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_results(simulation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a021466-1f6b-443f-9c28-59b969cc7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Saving the simulation results\n",
    "\n",
    "def save_results_llm(results, llm_name, filename_prefix=\"simulation_results\"):\n",
    "    \"\"\"\n",
    "    Save the simulation results for a specific LLM to a file.\n",
    "\n",
    "    Parameters:\n",
    "        results (dict): Dictionary containing aggregated results for each batch size.\n",
    "        llm_name (str): Name of the LLM being tested.\n",
    "        filename_prefix (str): Prefix for the file to save the results.\n",
    "    \"\"\"\n",
    "    save_data = {\n",
    "        'batch_sizes': [],\n",
    "        'OPT_SE': [], 'OPT_EE': [],\n",
    "        'PROP_SE': [], 'PROP_EE': [],\n",
    "        'RAND_SE': [], 'RAND_EE': [],\n",
    "        'BIN_SE': [], 'BIN_EE': []\n",
    "    }\n",
    "\n",
    "    for batch_size, metrics in results.items():\n",
    "        save_data['batch_sizes'].append(batch_size)\n",
    "        for metric in ['OPT_SE', 'OPT_EE', 'PROP_SE', 'PROP_EE', 'RAND_SE', 'RAND_EE', 'BIN_SE', 'BIN_EE']:\n",
    "            save_data[metric].append(metrics[metric])\n",
    "    \n",
    "    filename = f\"{filename_prefix}_{llm_name}.npz\"\n",
    "    np.savez(filename, **save_data)\n",
    "    print(f\"Results for {llm_name} saved to {filename}\")\n",
    "\n",
    "\n",
    "# Save the results\n",
    "save_results_llm(simulation_results,llm_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b0516-9937-4e21-850c-045f8284d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Loading the saved results (Optional)\n",
    "\n",
    "def load_results(filename=\"simulation_results.npz\"):\n",
    "    \"\"\"\n",
    "    Load the simulation results from a file.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Name of the file to load the results from.\n",
    "\n",
    "    Returns:\n",
    "        results (dict): Loaded simulation results.\n",
    "    \"\"\"\n",
    "    data = np.load(filename)\n",
    "    results = {}\n",
    "    \n",
    "    for i, batch_size in enumerate(data['batch_sizes']):\n",
    "        results[int(batch_size)] = {\n",
    "            'OPT_SE': float(data['OPT_SE'][i]),\n",
    "            'OPT_EE': float(data['OPT_EE'][i]),\n",
    "            'PROP_SE': float(data['PROP_SE'][i]),\n",
    "            'PROP_EE': float(data['PROP_EE'][i]),\n",
    "            'RAND_SE': float(data['RAND_SE'][i]),\n",
    "            'RAND_EE': float(data['RAND_EE'][i]),\n",
    "            'BIN_SE': float(data['BIN_SE'][i]),\n",
    "            'BIN_EE': float(data['BIN_EE'][i])\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "loaded_results = load_results(\"simulation_results_\"+llm_name+\".npz\")\n",
    "display_results(loaded_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bd932-8ba4-4eb7-8bbe-f2ca42137972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Additional analysis or visualization (Optional)\n",
    "\n",
    "# You can add any additional analysis or visualization here based on the simulation results.\n",
    "# For example, comparing SE and EE across different strategies.\n",
    "\n",
    "# Example: Comparing OPT_SE vs PROP_SE\n",
    "def compare_SE(results):\n",
    "    \"\"\"\n",
    "    Compare OPT_SE and PROP_SE across batch sizes.\n",
    "\n",
    "    Parameters:\n",
    "        results (dict): Dictionary containing aggregated results for each batch size.\n",
    "    \"\"\"\n",
    "    batch_sizes = sorted(results.keys())\n",
    "    OPT_SE = [results[bs][\"OPT_SE\"] for bs in batch_sizes]\n",
    "    PROP_SE = [results[bs][\"PROP_SE\"] for bs in batch_sizes]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(batch_sizes, OPT_SE, marker='o', label='OPT_SE')\n",
    "    plt.plot(batch_sizes, PROP_SE, marker='s', label='PROP_SE')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Spectral Efficiency (SE)')\n",
    "    plt.title('Comparison of OPT_SE and PROP_SE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Compare SE\n",
    "compare_SE(simulation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f22159d-6153-4a96-b6bc-d9273d755760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01370249-a919-4c0e-ae64-0f0892ca5273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
